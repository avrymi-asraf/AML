{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avrymi-asraf/AML/blob/main/Ex3/Ex3-interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGWYdQqm4El"
      },
      "source": [
        "# Imports and tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import re\n",
        "import math"
      ],
      "metadata": {
        "id": "FuyL6J_Gm9XF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown , clear_output, display\n",
        "\n",
        "md = lambda x: display(Markdown(x))\n",
        "he_md = lambda x: display(Markdown(f'<div dir=\"rtl\" lang=\"he\" xml:lang=\"he\">{x}</div>'))"
      ],
      "metadata": {
        "id": "aO24QkWioByi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "ilJTYwEipFKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-B_uSjzGm4Em"
      },
      "outputs": [],
      "source": [
        "# for colab\n",
        "# !mkdir src\n",
        "# !wget -O /content/src/augmentations.py https://raw.githubusercontent.com/avrymi-asraf/AML/main/Ex3/src/augmentations.py\n",
        "# !wget -O /content/src/data_load.py https://raw.githubusercontent.com/avrymi-asraf/AML/main/Ex3/src/data_load.py\n",
        "# !wget -O /content/src/models.py https://raw.githubusercontent.com/avrymi-asraf/AML/main/Ex3/src/models.py\n",
        "# !wget -O /content/src/vicreg_objectives.py https://raw.githubusercontent.com/avrymi-asraf/AML/main/Ex3/src/vicreg_objectives.py\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vo41yKz5m4En"
      },
      "outputs": [],
      "source": [
        "# for local machine\n",
        "from src.models import Encoder, Projector\n",
        "from src.vicreg_objectives import vicreg_loss_detailed, vicreg_loss_performance\n",
        "from src.data_load import (\n",
        "    load_cifar10,\n",
        "    load_mnist,\n",
        "    load_combined_test_set,\n",
        "    load_vicreg_cifar10,\n",
        "    test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO6GlPxvm4Eo"
      },
      "source": [
        "# Q1: Training\n",
        "Train VICReg on the CIFAR10 dataset. Plot the values of each of the 3 objectives (in separate\n",
        "figures) as a function of the training batches. In your figures also include the loss terms values on the test set,computed once every epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VICeg(nn.Module):\n",
        "    def __init__(self,en_dim=128,prj_dim=512):\n",
        "        super(VICeg, self).__init__()\n",
        "        self.encoder = Encoder(D=en_dim,device=DEVICE)\n",
        "        self.projector = Projector(D=en_dim,prj_dim = prj_dim, device=DEVICE)\n",
        "    def forward(self, x):\n",
        "        return self.projector(self.encoder(x))"
      ],
      "metadata": {
        "id": "llvgNfuHo22s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hyper Parameters VICeg\n",
        "model = VICeg()"
      ],
      "metadata": {
        "id": "MW3z1zqToLVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzd3L155m4Ep"
      },
      "source": [
        "# Q2: PCA vs. T-SNE Visualizations.\n",
        "Compute the representations of each test image using your trained encoder.\n",
        "Map (using the sklearn library) the representation to a 2D space using: (i) PCA (ii) T-SNE. Plot the T-SNE and the PCA 2D representations, colored by their classes. Look at both visualizations (PCA vs. T-SNE), which one seems more effective for visualizations to you? Look at the T-SNE visualization. Did VICReg managed to capture the class information accurately? Which classes seem entangled to you? Explain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxIZq1Ofm4Ep"
      },
      "source": [
        "# Q3: Linear Probing.\n",
        "Perform a linear probing (single FC layer) to the encoder’s representation. Train this\n",
        "classifier on the representations of the CIFAR10 train set. Remember to freeze the encoder, i.e. do not update it. Compute the probing’s accuracy on the test set. What is the accuracy you reach with your classifier?\n",
        "Note: classifier accuracy should be at least 60% on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVp9vMA2m4Ep"
      },
      "source": [
        "# Q4: Ablation 1 - No Variance Term.\n",
        "Modify the optimized objective, by removing the variance objective term\n",
        "(µ = 0.). Using the representations from the modified encoder, perform the same PCA visualization from Q2, and the linear probing from Q3 (and include them in your report). Is the new accuracy better or worse? Can you see anything different in the representations visualization? Try to explain the difference in the accuracy using the visualizations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFOvED5jm4Eq"
      },
      "source": [
        "# Q5: Ablation 2 - No Generated Neighbors.\n",
        "Now, we would like to ablate VICReg by only removing the generated neighbors, using neighbors from the data itself:\n",
        "First, compute the representations of your original VICReg, on all of the training set. In each step of training and for each image in the batch, use these representations to find the top 3 nearest neighbors, and randomly select 1 of them. Use the original image and this neighbor of it as your 2 views for the VICReg algorithm.\n",
        "\n",
        "2 Practical Tips: (i) We find that training this algorithm for only a single epoch is more\n",
        "beneficial. (ii) We recommend you to compute the neighboring indices of each image in advance, and delete the original VICReg model from your (GPU) memory. This will save both run time and GPU space.\n",
        "Compute the linear probing accuracy, and report it. Is the accuracy different from the original linear probing from 3?\n",
        "- If no, explain why do you think this change had no effect (what compensates the things that are missing?).\n",
        "- If yes, explain what added value do you think the generated neighbors adds to the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18u4UDsfm4Eq"
      },
      "source": [
        "# Q6: Ablation 3 - Laplacian Eigenmaps.\n",
        "After removing the generated neighbors, we would like to remove both it and the amortization at once. To do so, we will perform Laplacian Eigenmaps representation learning on the training data of CIFAR10. Since this algorithm is difficult to run, we ran it for you on 10K images (due to runtime limitations) and give you the T-SNE plotting of these representations in Fig. 2 3.\n",
        "Compare this to VICReg’s T-SNE plot from Q2. Based on this visual and linear probing comparison, which method (VICReg vs. Laplacian Eigenmaps) seems more effective for downstream object classification?\n",
        "Explain your answer in detail, including what do you think makes one algorithm to be more successful.\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQI_Xd5Bm4Eq"
      },
      "source": [
        "# Q7: Retrieval Evaluation\n",
        "Now that we slowly “pealed” VICReg back to the laplacian eigenmaps algorithm,\n",
        "we wish to evaluate it qualitatively. For the methods Q1 (VICReg) and Q5 (No Generated Neighbors) perform a\n",
        "qualitative retrieval evaluation. That means:\n",
        "- Select 10 random images from the training set, one from each class.\n",
        "- For each selected image, use the representations of each of the evaluated methods, to find its 5 nearest neighbors\n",
        "in the dataset.\n",
        "- Plot the images together with their neighbors.\n",
        "- Using the same images, perform the same visualization for the 5 most distant images in the dataset.\n",
        "\n",
        "Using this visualization, explain what attributes each method attends to. What are the differences you see between\n",
        "the different methods? Which one excels at keeping close images together? Which one excels at keeping distant\n",
        "images far apart? Explain the differences between the methods in detail, as seen by this visualization. You may\n",
        "select more than 1 image for a specific class if you wish to get a better understanding (Although it is not mandatory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGPJzeTrm4Eq"
      },
      "source": [
        "# Q1 - Anomaly Detection.\n",
        "Using the CIFAR10 training data as reference for normal data, compute the kNN\n",
        "density estimation for all the (CIFAR10 + MNIST) test set representations. Do this for both (i) VICReg (ii) VICReg\n",
        "without generated neighbors. Use k = 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHY2S9ALm4Eq"
      },
      "source": [
        "# Q2 - ROC AUC Evaluation\n",
        "Plot the ROC Curve of both methods. Use the sklearn library for creating these\n",
        "figures. In the title / legend incorporate the AUC of each method. Which method is ’better’? In a sentence or two,\n",
        "explain why do you think its better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUmDPT5rm4Eq"
      },
      "source": [
        "# Q3 - Qualitative Evaluation - Ambiguity of Anomaly Detection.\n",
        "Plot the 7 most anomalous samples according\n",
        "to VICReg and VICReg without the generated neighbors, in two separate rows (you can split to different plots if\n",
        "more convenient). Look at the results. Explain what aspects each method found to be anomalous. Keeping in mind\n",
        "we did not give either of the methods any clues regarding which anomalies are we looking for, do you still think one\n",
        "is better than the other? Explain your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxnrN1Ccm4Eq"
      },
      "source": [
        "# Q1 - Clustering using K-Means\n",
        "Use the implementation of sklearn to cluster the CIFAR10 training set to\n",
        "10 clusters. Do this using the representations of VICReg once, and a second time for VICReg without generated\n",
        "neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eKGKEzYm4Eq"
      },
      "source": [
        "# Q2 - Visualizing the Clusters in 2D\n",
        "Perform a dimensionality reduction to 2 dimensions using T-SNE, for\n",
        "the representations of both methods (separately). For each method, plot the reduced embeddings in a 2D space twice\n",
        "(side by side): (i) First, colored by their cluster index (according to the matching clustering from Q1). (ii) Second,\n",
        "colored by their actual class index. In each figure, also plot the clusters centers in black color.\n",
        "Look at the results, which method looks more successful at finding clusters? Which method looks more successful\n",
        "at separating between the classes? Explain your answer, keeping in mind that similarly to Anomaly Detection there\n",
        "are no clues for which clusters to look for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTaczS0Nm4Er"
      },
      "source": [
        "# Q3 - Quantitative Analysis\n",
        "Use the Silhouette Score on the clusterings of both methods (in their original embedding dimensions). Report the 2 different scores. Is this coherent with what you see in your visual analysis? Explain your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76m0m-Xzm4Er"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AML-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}